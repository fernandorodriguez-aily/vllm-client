{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook shows how we can use a VLLM server that follows OpenAI RESTful API. It considers:\n",
    "\n",
    "* **Using directly the OpenAI library.** It has the advantage that we can add extra parameters supported specifically by VLMM but not by the \"traditional\" OpenAI API. They are introduced in the `extra_body` part of the request.\n",
    "\n",
    "* **Using LangChain with `ChatOpenAI`**. In this case, we need to follow the traditional approach, we have not been able to use the extra features of  VLLM\n",
    "\n",
    "**Note on LoRA adapters:** I think ChatOpenAI should work with LoRA adapters because they are considered \"another model\" of the server. We probably need one docker image per model, with its set of associated LoRA adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI to directly interact with the VLLM server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"token-abc123\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"NousResearch/Meta-Llama-3-8B-Instruct\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra parameters for Chat API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[vLLM supports a set of parameters that are not part of the OpenAI API. In order to use them, you can pass them as extra parameters in the OpenAI client. Or directly merge them into the JSON payload if you are using HTTP call directly.](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#extra-parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without `guided_choice`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='The sentiment in the statement \"vLLM is wonderful!\" is POSITIVE.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"NousResearch/Meta-Llama-3-8B-Instruct\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Classify this sentiment: vLLM is wonderful!\"}\n",
    "  ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With `guided_choice`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='positive', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"NousResearch/Meta-Llama-3-8B-Instruct\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Classify this sentiment: vLLM is wonderful!\"}\n",
    "  ],\n",
    "  extra_body={\n",
    "    \"guided_choice\": [\"positive\", \"negative\"]\n",
    "  }\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangChain to interact with the VLLM server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "inference_server_url = \"http://localhost:8000/v1\"\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"NousResearch/Meta-Llama-3-8B-Instruct\",\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base=inference_server_url,\n",
    "    max_tokens=100,\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The translation of the sentence \"I love programming\" from English to Italian is:\\n\\n\"Mi piace programmazione.\"\\n\\nHere\\'s a breakdown of the translation:\\n\\n* \"I\" is translated to \"Mi\"\\n* \"love\" is translated to \"piace\"\\n* \"programming\" is translated to \"programmazione\"', response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 40, 'total_tokens': 105}, 'model_name': 'NousResearch/Meta-Llama-3-8B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-eed052d5-9214-4fdb-b96b-0ade934b60bd-0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant that translates English to Italian.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Translate the following sentence from English to Italian: I love programming.\"\n",
    "    ),\n",
    "]\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ti piace il programming!', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 31, 'total_tokens': 38}, 'model_name': 'NousResearch/Meta-Llama-3-8B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-801bdad5-8d97-4462-8734-81ec6cc20c39-0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "template = (\n",
    "    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    ")\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "chat(\n",
    "    chat_prompt.format_prompt(\n",
    "        input_language=\"English\", output_language=\"Italian\", text=\"I love programming.\"\n",
    "    ).to_messages()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I have not been able to call the server with extra parameters when using LangChain**. If we want all of the extra features that VLLM offers, we would probably need to extend ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without `guided_choice`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I would classify this sentiment as POSITIVE. The use of the word \"wonderful\" is a strong positive adjective that expresses enthusiasm and admiration for the topic being referred to, which is vLLM.', response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 33, 'total_tokens': 76}, 'model_name': 'NousResearch/Meta-Llama-3-8B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1f5f9ec9-646f-4f32-8f52-45f2645e07ef-0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"You are a helpful assistant. Classify this sentiment: vLLM is wonderful!\"\n",
    "    ),\n",
    "]\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With `guided_choice`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try #1 (kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1532967881.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[35], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    chat.invoke(input=messages, **kwargs=extra_body)\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content=\"You are a helpful assistant. Classify this sentiment: vLLM is wonderful!\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "extra_body ={\n",
    "    \"guided_choice\": [\"positive\", \"negative\"]\n",
    "  }\n",
    "\n",
    "chat.invoke(input=messages, kwargs=extra_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try #2 (configurable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I would classify this sentiment as POSITIVE. The use of the word \"wonderful\" is a strong positive adjective that expresses enthusiasm and admiration for the topic being referred to, which is vLLM.', response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 33, 'total_tokens': 76}, 'model_name': 'NousResearch/Meta-Llama-3-8B-Instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d3c8399b-ba31-4481-a326-009dc8c1da76-0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.with_config(\n",
    "    configurable={\"guided_choice\": [\"positive\", \"negative\"]}\n",
    ").invoke(messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
